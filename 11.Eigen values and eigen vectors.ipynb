{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Eigen values, Eigen vectors, Eigen space",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "# Eigen values\n- A scalar (or factor instead) say $\\lambda$ is an eigen value if $T(x)=\\lambda x$\n- Intutively a scaling factor is enough to describe a linear transform, then it is called an eigen value.\n- As the discussion is more calculative, we need matrices into picture to make the problem of finding eigen value as a problem of finding solution of an equation.\n- Let $A$ be a square matrix, and $I$ be the identity matrix then the eigen values $\\lambda$ of $A$ are the roots of the equation $\\text{det}(A-\\lambda I)=0$ \n- $\\text{det}(A-\\lambda I)=0$ is called the characteristic equation, and so the roots are called characteristics root.\n- Spectral values, latent roots, eigen values mean the same thing.\n- Eigen value of $m\\times m$ is $m$ in numbers except for multiplicity.\n- The product of eigen values gives the determinant of matrix.\n- The sum of eigen values gives the trace of matrix.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Finding eigen value \n- Example $A=\\begin{pmatrix}1&3\\\\ 4&2\\end{pmatrix}$\n$$|A-\\lambda I|=0$$\nor,\n$$\\begin{vmatrix}1&3\\\\ 4&2\\end{vmatrix}-\\lambda \\begin{vmatrix}1&0\\\\ 0&1\\end{vmatrix}=0$$\nor,\n$$\\begin{vmatrix}1-\\lambda &3\\\\ 4&2-\\lambda\\end{vmatrix}=0$$\nor,\n$$(1-\\lambda)(2-\\lambda)-3\\cdot 4 =0$$\n$$2-3\\lambda+{\\lambda}^2-12 =0$$\n$${\\lambda}^2-3\\lambda-10 =0$$\n$${\\lambda}^2-5\\lambda+2\\lambda-10 =0$$\n$${\\lambda}(\\lambda-5)+2(\\lambda-5) =0$$\n$${(\\lambda+2)}(\\lambda-5) =0$$\nSo, finally we have $\\lambda=-2,5$, or we just write that the eigen values are $\\lambda_1=-2,\\lambda_2=5$\n\n\n\n\n",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# find the eigen values of [1,3],[4,2]\nimport numpy as np\nA=np.array([[1,3],\n            [4,2]])\nI=np.identity(2)\n#np.linalg.eig(A)\nval,vec=np.linalg.eig(A)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "source": "val",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "execution_count": 2,
          "output_type": "execute_result",
          "data": {
            "text/plain": "array([-2.,  5.])"
          },
          "metadata": {}
        }
      ],
      "execution_count": 2
    },
    {
      "cell_type": "code",
      "source": "vec",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "execution_count": 3,
          "output_type": "execute_result",
          "data": {
            "text/plain": "array([[-0.70710678, -0.6       ],\n       [ 0.70710678, -0.8       ]])"
          },
          "metadata": {}
        }
      ],
      "execution_count": 3
    },
    {
      "cell_type": "markdown",
      "source": "## Eigen vectors\n- Let $\\lambda$ be an eigen vector, then the null space of $A-\\lambda I$ is the eigen vector for the eigen value $\\lambda$ ",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "From the previous example we have eigen values of $A=\\begin{pmatrix}1&3\\\\ 4&2\\end{pmatrix}$ as $-2,5$\n- now we compute eigen vectors as solution of:\n$(A-\\lambda I) x=\\begin{pmatrix}1-\\lambda&3\\\\ 4&2-\\lambda\\end{pmatrix}\\begin{pmatrix}x_1\\\\ x_2\\end{pmatrix}=\\begin{pmatrix}0\\\\ 0\\end{pmatrix}$\n- now, we have to put $\\lambda=-2$\n- We now have $\\begin{pmatrix}1+2&3\\\\ 4&2+2\\end{pmatrix}\\begin{pmatrix}x_1\\\\ x_2\\end{pmatrix}=\\begin{pmatrix}0\\\\ 0\\end{pmatrix} \\implies \\begin{pmatrix}3&3\\\\ 4&4\\end{pmatrix}\\begin{pmatrix}x_1\\\\ x_2\\end{pmatrix}=\\begin{pmatrix}0\\\\ 0\\end{pmatrix}\\implies (x_1,x_2)=(1,-1)$\n- now, we have to put $\\lambda=5$ to get $\\begin{pmatrix}1-5&3\\\\ 4&2-5\\end{pmatrix}\\begin{pmatrix}x_1\\\\ x_2\\end{pmatrix}=\\begin{pmatrix}0\\\\ 0\\end{pmatrix} \\implies \\begin{pmatrix}-4&3\\\\ 4&-3\\end{pmatrix}\\begin{pmatrix}x_1\\\\ x_2\\end{pmatrix}=\\begin{pmatrix}0\\\\ 0\\end{pmatrix} \\implies (x_1,x_2)=(3,4)$\n- The eigen vectors are $(1,-1),(3,4)$",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "np.identity(3)",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "execution_count": 6,
          "output_type": "execute_result",
          "data": {
            "text/plain": "array([[1., 0., 0.],\n       [0., 1., 0.],\n       [0., 0., 1.]])"
          },
          "metadata": {}
        }
      ],
      "execution_count": 6
    },
    {
      "cell_type": "markdown",
      "source": "# Eigen decomposition\n- It is the process of obtaining the most simpler matrix $D$ called the diagonal matrix for a matrix $A$.\n- It is popularly known as diagonalization process istead of eigen decomposition.\n- $A=XDX^{-1}$ is the required decompposition.\n- Here, $D$ is the diagonal matrix for A.\n- $X$ is the matrix of the eigen vectors of $A$.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "**For example** we need to diagonalize the matrix\n$A=\\begin{pmatrix}1&3\\\\ 4&2\\end{pmatrix}$ and as we know the eigen vectors are $(1,-1),(3,4)$\nso we have, $X=\\begin{pmatrix}1&3\\\\-1&4\\end{pmatrix}$.\n\nNow we need inverse of $X$ to be muliplied to get the diagonal matrix as $\\begin{pmatrix}1&3\\\\ 4&2\\end{pmatrix}$ and as we know the eigen vectors are $(1,-1),(3,4)$\nso we have, $D=\\begin{pmatrix}-2&0\\\\0&5\\end{pmatrix}$",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import numpy as np\n\n# Step 1: Defining the matrix\nA = np.array([[1, 3],\n              [4, 2]])\n# Step 2: Finding eigenvalues and eigenvectors\neigenvalues, eigenvectors = np.linalg.eig(A)\n\n# Step 3: Check if the matrix is diagonalizable\nif not np.all(np.iscomplex(eigenvalues)):\n    # Step 4: Create the diagonal matrix\n    D = np.diag(eigenvalues)\n\n    # Step 5: Find the inverse of the eigenvector matrix\n    P_inv = np.linalg.inv(eigenvectors)\n\n    # Step 6: Diagonalize the matrix\n    diagonalized_matrix = np.dot(P_inv, np.dot(A, eigenvectors))\n\n    # Printing the diagonalized matrix\n    print(\"Diagonalized Matrix:\")\n    print(diagonalized_matrix)\nelse:\n    print(\"Matrix is not diagonalizable.\")\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Diagonalized Matrix:\n[[-2.0000000e+00  4.4408921e-16]\n [ 0.0000000e+00  5.0000000e+00]]\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 19
    }
  ]
}